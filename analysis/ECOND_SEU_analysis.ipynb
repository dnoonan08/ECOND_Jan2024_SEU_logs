{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c7c343-2ff1-4a43-b020-d70089c67ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"firstcell.py\"\n",
    "import numpy\n",
    "import matplotlib\n",
    "from matplotlib import pylab, mlab, pyplot\n",
    "np = numpy\n",
    "plt = pyplot\n",
    "\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "\n",
    "from pylab import *\n",
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6041c65c-545a-4535-a90b-39c956ec0f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cycler import cycler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82df73ad-c49e-4130-80ec-9a7fedcbbae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import json\n",
    "import bitstruct\n",
    "import crcmod\n",
    "import scipy.stats\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "from ECOND_SEU import run_times, fluences, hexa42_files, hexa43_files, hexprint, yerr, pois_err, items, packet_parser, capture_batch, incident, run\n",
    "from ECOND_SEU import PRBS_run_times, PRBS_fluences, hexa42_PRBS_files, hexa43_PRBS_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140575f0-4315-48c5-978b-6061ebb2dc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "hexa_files_location = '/home/jsw/tmp/ECOND_Jan2024_SEU_logs/logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e687616-f6c1-4605-b24d-9baa85061034",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "J43 = []\n",
    "percaps = []\n",
    "for i, fname in enumerate(hexa43_files):\n",
    "    if i < 9:\n",
    "        percaps.append(8)\n",
    "    else:\n",
    "        percaps.append(5)\n",
    "    with open(pathlib.Path(hexa_files_location, fname)) as jsonfile:\n",
    "        J43.append(json.load(jsonfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76b828c-d6eb-4045-85e4-4a62a50f3c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "J42 = []\n",
    "percaps = []\n",
    "for i, fname in enumerate(hexa42_files):\n",
    "    if i < 9:\n",
    "        percaps.append(8)\n",
    "    else:\n",
    "        percaps.append(5)\n",
    "    with open(pathlib.Path(hexa_files_location, fname)) as jsonfile:\n",
    "        J42.append(json.load(jsonfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac56329-a93f-4768-906f-6cfa10d67680",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prebeam = packet_parser(J42[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e949d8a-a05a-4676-8de5-583282dc9edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "R42 = [run(J42[k], percaps[k], prebeam) for k in trange(16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8501a512-4330-45ee-a1e9-65fa00a9ba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "R43 = [run(J43[k], percaps[k], prebeam) for k in trange(16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf3e44-886a-4013-b5cf-e031a2b4b1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(\"all58.npz\") as f:\n",
    "    patt5 = f['all5']\n",
    "    patt8 = f['all8']\n",
    "    e_bits = f['e_bits']\n",
    "    e_words = f['e_words']\n",
    "    e_BXs = f['e_BXs']\n",
    "    e_specs = f['e_specs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db526019-6fd8-4ff1-84ec-a1aaec2311b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('false_pileup.npz') as f:\n",
    "    double_patt8 = f['double_patt8']\n",
    "    double_i8 = f['double_i8']\n",
    "    double_j8 = f['double_j8']\n",
    "    total_double8 = f['total8']\n",
    "    double_patt5 = f['double_patt5']\n",
    "    double_i5 = f['double_i5']\n",
    "    double_j5 = f['double_j5']\n",
    "    total_double5 = f['total5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef33b34-0355-4508-a14f-a32dfd398300",
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('OB_double_SEU.npz') as f:\n",
    "    OB_BX_offset_patt8 = f['OB_BX_offset_patt8']\n",
    "    OB_bit_offset_patt8 = f['OB_bit_offset_patt8']\n",
    "    OB_bit_offset_valid8 = f['OB_bit_offset_valid8']\n",
    "    OB_bit_offset_patt5 = f['OB_bit_offset_patt5']\n",
    "    OB_bit_offset_valid5 = f['OB_bit_offset_valid5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fabe86-5876-4a0b-b575-835d090ec324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC_patt5 = unique(concatenate([patt5, double_patt5, OB_bit_offset_patt5[OB_bit_offset_valid5]]))\n",
    "# MC_patt8 = unique(concatenate([patt8, double_patt8, OB_bit_offset_patt8[OB_bit_offset_valid8], OB_BX_offset_patt8]))\n",
    "MC_patt5 = unique(concatenate([patt5, double_patt5]))\n",
    "MC_patt8 = unique(concatenate([patt8, double_patt8]))\n",
    "\n",
    "len(MC_patt5), len(MC_patt8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef77963d-3d79-4a18-9af6-784d1cb38554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_PP_indices():\n",
    "    # These four lines come directly from test_proton_seu.py\n",
    "    np.random.seed(15665843)\n",
    "    L1A_count=67\n",
    "    l1a_bx=np.random.choice(np.arange(10,3450),L1A_count,replace=False)\n",
    "    l1a_bx.sort()\n",
    "\n",
    "    # Now we need to offset the start time enough to finish preceding\n",
    "    # packets, and further offset it by 54 BX to make it match up with\n",
    "    # the error data.\n",
    "    prev_l = -inf\n",
    "    ls = []\n",
    "    for l in l1a_bx:\n",
    "        newl = max(l, prev_l + 41)\n",
    "        ls.append(newl+54)\n",
    "        prev_l = newl\n",
    "    ls = array(ls)\n",
    "    ls[52] += 1 # Not sure why this is necessary, but it makes the analysis line up properly.\n",
    "\n",
    "    # And then we need to find the distance to the immediately\n",
    "    # preceding PP readout start for every BX, wrapping around\n",
    "    # the end of the orbit as needed.\n",
    "    return ((arange(3564) - ls[(searchsorted(ls, arange(3564), side='right') - 1) % len(ls)]) % 3564)\n",
    "PP_indices = calc_PP_indices()\n",
    "\n",
    "def PP_reads():\n",
    "    # Start from 0, -4 to accommodate the packet header\n",
    "    x = 0\n",
    "    y = -4\n",
    "    for i in range(472):\n",
    "        yield x, y, i//12\n",
    "        if y >= 0:\n",
    "            x += 1\n",
    "        y += 1\n",
    "        if x == 12:\n",
    "            x = 0\n",
    "        if y == 37:\n",
    "            # Go to -2 to accommodate the subpacket headers\n",
    "            y = -2\n",
    "    # Note that the last two (of 472) have negative y. This is right\n",
    "    # because this is the packet trailer.\n",
    "\n",
    "# This is the amount to add to the difference between the write and read times\n",
    "PP_baseline_latency = 40\n",
    "    \n",
    "# Make this big enough to accommodate the largest of PP_indices\n",
    "PP_dwell = zeros((12,300), int)\n",
    "for x, y, BX in PP_reads():\n",
    "    # BX is the read out time\n",
    "    # x is the RAM (and therefore the error word)\n",
    "    # y tells us the read in time\n",
    "    # if x or y is negative, then it's not really in the RAM so we ignore it\n",
    "    if (x >= 0) and (y >= 0):\n",
    "        PP_dwell[11-x, BX] = BX - y + PP_baseline_latency\n",
    "assert PP_dwell.min() >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9643286a-7bb6-439a-ba95-668847d654fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = []\n",
    "with open('/home/jsw/src/econd-sw/RAM_rw') as f:\n",
    "    for line in f:\n",
    "        l = line.strip()\n",
    "        if len(l) > 0:\n",
    "            RAstr, WAstr, REstr, WEstr, RDstr, WDstr = l.split(',')\n",
    "            if 'x' in WAstr: continue\n",
    "            RAs = [int(RAstr[2*i:2*(i+1)], 16) for i in range(12)]\n",
    "            WAs = [int(WAstr[2*i:2*(i+1)], 16) for i in range(12)]\n",
    "            REs = [(int(REstr,16) >> (11 - i)) & 0x1 for i in range(12)]\n",
    "            WEs = [int(WEstr) for i in range(12)]\n",
    "            RDs = [int(RDstr[8*i:8*(i+1)], 16) if 'x' not in RDstr[8*i:8*(i+1)] else -1 for i in range(12)]\n",
    "            WDs = [int(WDstr[8*i:8*(i+1)], 16) if 'x' not in WDstr[8*i:8*(i+1)] else -1 for i in range(12)]\n",
    "            dat.append([RAs, WAs, REs, WEs, RDs, WDs])\n",
    "            \n",
    "dat = array(dat)\n",
    "\n",
    "RAMt = zeros((12, 256), uint)\n",
    "RAM = zeros((12, 256), uint32)\n",
    "\n",
    "dwells = []\n",
    "j = 0\n",
    "q = 0\n",
    "prev_dwell = 45\n",
    "for i in range(dat.shape[0]-1):\n",
    "    RAs, WAs, REs, WEs, RDs, WDs = dat[i]\n",
    "    for k in range(12):\n",
    "        if WEs[k] == 0:\n",
    "            RAMt[k,WAs[k]] = i\n",
    "            RAM[k, WAs[k]] = WDs[k]\n",
    "    if any(REs == 0):\n",
    "        while True:\n",
    "            if REs[j] == 0:\n",
    "                dwellt = uint64(i) - RAMt[j, RAs[j]]\n",
    "                if dwellt < 10 and prev_dwell > 10:\n",
    "                    # Start of new packet\n",
    "                    q = 0\n",
    "                else:\n",
    "                    q = (q + 1) % 6\n",
    "                prev_dwell = dwellt\n",
    "                dwells.append((dwellt, RAMt[j,RAs[j]], \n",
    "                               i, j, RAs[j], q, RAM[j,RAs[j]], dat[i+1,4,j]))\n",
    "                j = (j + 1) % 12\n",
    "            else:\n",
    "                break\n",
    "dwells = array(dwells, int64)\n",
    "\n",
    "# dwell_splits = (diff(1*(dwells[:,0] < 10)) == 1).nonzero()[0]\n",
    "\n",
    "assert all(repeat(roll(dwells, 278, axis=0)[:,5], 32) == e_words[(e_specs == 2)])\n",
    "\n",
    "assert all(repeat((roll(dwells, 278, axis=0)[:,2] + 57)%3564, 32) == e_BXs[(e_specs == 2)])\n",
    "\n",
    "OB_dwell = zeros_like(e_specs)\n",
    "OB_dwell[e_specs == 2] = repeat(roll(dwells, 278, axis=0)[:,0], 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a90d932-0a8c-4d23-8446-5f1d7ccbe004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the eRX, the MSB (bit 31) comes in first, so it has the longest dwell\n",
    "# time in untriplicated registers.  Each subsequent bit should have a dwell\n",
    "# time that is 1 more cycle of a 1280 MHz clock.  There may be some baseline\n",
    "# offset, such that bit 0 has a dwell time of a few cycles of the 1280 MHz\n",
    "# clock, so we'll include that here as well, with an initial guess of 4.\n",
    "spec0_deser_baseline = 31\n",
    "spec0_align_baseline = 4\n",
    "spec0_FF_dwelltime = ((e_bits+1)//2 + spec0_deser_baseline + spec0_align_baseline) * 25e-9 / 32\n",
    "spec0_LATCH_dwelltime = (e_bits // 2) * 25e-9 / 32\n",
    "\n",
    "# These values are taken from the tagged code that we used on Jan 27:\n",
    "# https://gitlab.cern.ch/hgcal-daq-sw/econd-sw/-/blob/SEU_test_Jan2024_v2/test_radiation/conftest.py#L247-250\n",
    "phase_select_42 = array([ 5,  5,  5,  5,  6,  7,  7,  8,  8,  2,  2,  2], uint8)\n",
    "phase_select_43 = array([ 5,  12,  5, 12,  6,  7,  7,  8,  7, 10,  2,  2], uint8)\n",
    "# Phase select has 15 possible values for a DLL\n",
    "# The total length of the DLL is two entire bit intervals, or 2 * 25e-9 / 32 nanoseconds\n",
    "# So, the dwell time in the delay line in nanoseconds is phase_select * (2 * 25e-9 / 32) / 15\n",
    "spec0_DLL_dwelltime42 = (phase_select_42[e_words] + 0.5) * (2 * 25e-9 / 32) / 15\n",
    "spec0_DLL_dwelltime43 = (phase_select_43[e_words] + 0.5) * (2 * 25e-9 / 32) / 15\n",
    "\n",
    "# In the Ping-Pong RAM, the dwell time is always in an integer number of\n",
    "# 40 MHz clock cycles.\n",
    "spec1_dwelltime = PP_dwell[11-e_words, PP_indices[e_BXs]] * 25e-9\n",
    "\n",
    "# We calculated the dwell time using simulation of the ECON-D ASIC\n",
    "spec2_dwelltime = OB_dwell * 25e-9\n",
    "\n",
    "# In the serializer, bit 31 (the MSB) lives in a 32-bit register clocked at\n",
    "# 40 MHz for 1 cycle of a 640 MHz clock, then it lives in a 2-bit register\n",
    "# clocked at 640 MHz for 1 cycle of a 1280 MHz clock, then it lives in a 1-bit\n",
    "# register clocked at 1280 MHz for 1 cycle of a 1280 MHz clock.  So it lives\n",
    "# for a total of 4 cycles of the 1280 MHz clock in untriplicated registers.\n",
    "# Bit 30 lives for 5 cycles of the 1280 MHz clock, bit 29 for 6 cycles, etc.,\n",
    "# down to bit 0 which lives for 35 cycles of the 1280 MHz clock in\n",
    "# untriplicated registers.\n",
    "spec3_baseline = 4\n",
    "spec3_dwelltime = ((31 - e_bits) + spec3_baseline) * 25e-9 / 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52348ff3-8aa6-4898-977f-8be2a95f916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_sigma = 4\n",
    "dwelltime = zeros((N_sigma, e_specs.shape[0]), dtype=float)\n",
    "dwelltime[0] = spec0_LATCH_dwelltime * (e_specs == 0)\n",
    "dwelltime[0] += spec0_FF_dwelltime * (e_specs == 0)\n",
    "dwelltime[1] = spec1_dwelltime * (e_specs == 1)\n",
    "dwelltime[2] = spec2_dwelltime * (e_specs == 2)\n",
    "dwelltime[3] = spec3_dwelltime * (e_specs == 3)\n",
    "\n",
    "dwelltime42 = copy(dwelltime)\n",
    "dwelltime42[0] += spec0_DLL_dwelltime42 * (e_specs == 0)\n",
    "\n",
    "dwelltime43 = copy(dwelltime)\n",
    "dwelltime43[0] += spec0_DLL_dwelltime43 * (e_specs == 0)\n",
    "\n",
    "\n",
    "\n",
    "# Convert dwelltime to fraction of an orbit\n",
    "dwelltime42 /= (3564 * 25e-9)\n",
    "dwelltime43 /= (3564 * 25e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea20c03-986c-49f1-b735-8023d6de8545",
   "metadata": {},
   "outputs": [],
   "source": [
    "D5_42 = zeros((N_sigma + N_sigma**2, MC_patt5.shape[0]), float)\n",
    "key = searchsorted(MC_patt5, patt5)\n",
    "for j in trange(patt5.shape[0]):\n",
    "    D5_42[:N_sigma,key[j]] += dwelltime42[:,j]\n",
    "double_key = searchsorted(MC_patt5, double_patt5)\n",
    "for k in trange(double_patt5.shape[0]):\n",
    "    D5_42[N_sigma:,double_key[k]] += (dwelltime42[:,newaxis,double_i5[k]] * dwelltime42[newaxis,:,double_j5[k]]).flatten() * sum(total_double5) / len(double_patt5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0359bee2-6c92-4bcb-bc35-98cddbedb459",
   "metadata": {},
   "outputs": [],
   "source": [
    "D8_42 = zeros((N_sigma + N_sigma**2, MC_patt8.shape[0]), float)\n",
    "key = searchsorted(MC_patt8, patt8)\n",
    "for j in trange(patt8.shape[0]):\n",
    "    D8_42[:N_sigma,key[j]] += dwelltime42[:,j]\n",
    "double_key = searchsorted(MC_patt8, double_patt8)\n",
    "for k in trange(double_patt8.shape[0]):\n",
    "    D8_42[N_sigma:,double_key[k]] += (dwelltime42[:,newaxis,double_i8[k]] * dwelltime42[newaxis,:,double_j8[k]]).flatten() * sum(total_double8) / len(double_patt8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc83f751-afb6-446e-aa35-6f6fbb7c356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "D5_43 = zeros((N_sigma + N_sigma**2, MC_patt5.shape[0]), float)\n",
    "key = searchsorted(MC_patt5, patt5)\n",
    "for j in trange(patt5.shape[0]):\n",
    "    D5_43[:N_sigma,key[j]] += dwelltime43[:,j]\n",
    "double_key = searchsorted(MC_patt5, double_patt5)\n",
    "for k in trange(double_patt5.shape[0]):\n",
    "    D5_43[N_sigma:,double_key[k]] += (dwelltime43[:,newaxis,double_i5[k]] * dwelltime43[newaxis,:,double_j5[k]]).flatten() * sum(total_double5) / len(double_patt5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86a471c-a2ea-4020-ad0c-9da8ef27bf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "D8_43 = zeros((N_sigma + N_sigma**2, MC_patt8.shape[0]), float)\n",
    "key = searchsorted(MC_patt8, patt8)\n",
    "for j in trange(patt8.shape[0]):\n",
    "    D8_43[:N_sigma,key[j]] += dwelltime43[:,j]\n",
    "double_key = searchsorted(MC_patt8, double_patt8)\n",
    "for k in trange(double_patt8.shape[0]):\n",
    "    D8_43[N_sigma:,double_key[k]] += (dwelltime43[:,newaxis,double_i8[k]] * dwelltime43[newaxis,:,double_j8[k]]).flatten() * sum(total_double8) / len(double_patt8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7c03bb-9a6d-4317-90ac-3b780a6b19f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_merges(D, Nbins = 200):\n",
    "    M = eye(D.shape[1])\n",
    "    max_acos = 2\n",
    "    Dnew = copy(D[:N_sigma+1])\n",
    "    Dnew[N_sigma] = D[N_sigma:].sum(axis=0)\n",
    "    while M.shape[0] > Nbins:\n",
    "        d = (asmatrix(M) * asmatrix(Dnew).T).A\n",
    "        d /= sqrt((d**2).sum(axis=1, keepdims=True))\n",
    "        A, B = triu_indices(d.shape[0], k=1)\n",
    "        cos_angle = sum(d[A] * d[B], axis=1) #/ sqrt(sum(d[A]**2, axis=1) * sum(d[B]**2, axis=1))\n",
    "        n = argmax(cos_angle)\n",
    "        max_cos_angle = cos_angle[n]\n",
    "        M[A[n]] += M[B[n]]\n",
    "        M = delete(M, B[n], axis=0)\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a148a4ab-294c-440a-9b87-82db4cc72317",
   "metadata": {},
   "outputs": [],
   "source": [
    "M8 = make_merges(D8_42 + D8_43, 25)\n",
    "M5 = make_merges(D5_42 + D5_43, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a18406f-c8e7-42c8-94d8-cb78a502bdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def analysis(R, plot_title, D, MC_patt, M, fluence, do_plot=False, fudge=1):\n",
    "    Ddata = zeros(MC_patt.shape[0], int32)\n",
    "    for i, p in enumerate(MC_patt):\n",
    "        Ddata[i] += sum(R.incidents.pattern == p)\n",
    "    \n",
    "    # N_sigma = 4\n",
    "    assert D.shape[0] == N_sigma * (N_sigma + 1)\n",
    "    \n",
    "    D = (asmatrix(M) * asmatrix(D).T).A.T * fluence\n",
    "    D1 = D[:N_sigma].T\n",
    "    D2 = D[N_sigma:].T.reshape(-1,N_sigma,N_sigma)\n",
    "    \n",
    "    Ddata = (asmatrix(M) * asmatrix(Ddata).T).A.flatten()\n",
    "    \n",
    "    tD1 = torch.tensor(D1, requires_grad=False)\n",
    "    tD2 = torch.tensor(D2, requires_grad=False)\n",
    "    tDdata = torch.tensor(Ddata, requires_grad=False)\n",
    "    \n",
    "    def tlambda(tbeta):\n",
    "        return ((tD1 * tbeta).sum(axis=1) +\n",
    "                fudge*fluence * (tD2 * torch.outer(tbeta, tbeta)).sum(axis=(1,2)) / (tD1 * tbeta).sum())\n",
    "    \n",
    "    def tlogpdf(tbeta):\n",
    "        return -torch.distributions.Poisson(tlambda(tbeta)).log_prob(tDdata).sum()\n",
    "\n",
    "    def logpdf(beta):\n",
    "        tbeta = torch.tensor(abs(beta))\n",
    "        return tlogpdf(tbeta).detach().numpy().item()\n",
    "\n",
    "    def jac(beta):\n",
    "        tbeta = torch.tensor(abs(beta), requires_grad=True)\n",
    "        lp = tlogpdf(tbeta)\n",
    "        lp.backward()\n",
    "        return tbeta.grad.detach().numpy()\n",
    "\n",
    "    def hesse(beta):\n",
    "        tbeta = torch.tensor(abs(beta), requires_grad=True)\n",
    "        return torch.autograd.functional.hessian(tlogpdf, tbeta).detach().numpy()\n",
    "    \n",
    "    res = scipy.optimize.minimize(logpdf, exp(randn(N_sigma)*0.7) * ones(N_sigma) * sum(Ddata) / sum(D1),\n",
    "                                  method='Nelder-Mead', options={'fatol': 1e-7, 'xatol':1e-19, 'maxiter': 5000, 'maxfev': 5000, 'disp':False})\n",
    "    # res = scipy.optimize.minimize(logpdf, res.x,\n",
    "    #                               jac=jac, hess=hesse, method='Newton-CG', options={'xtol': 1e-15, 'disp':True})\n",
    "    # print(res)\n",
    "    # print()\n",
    "    beta = abs(res.x)\n",
    "    H = hesse(beta)\n",
    "    Cov = linalg.inv(H)\n",
    "    # unc = sqrt(diag(asmatrix(D).T * asmatrix(Cov) * asmatrix(D)))\n",
    "    \n",
    "    tbeta = torch.tensor(abs(beta), requires_grad=False)\n",
    "    lamb = tlambda(tbeta)\n",
    "    # inds = argsort(lamb.detach().numpy())[::-1]\n",
    "    inds = argsort(sum(D1 * beta, axis=1) + sum(D2 * outer(beta, beta), axis=(1,2)) * fluence / sum(D1 * beta))[::-1]\n",
    "    # keep = ((Ddata > 0) | ((D*beta[:,newaxis]).sum(axis=0) > 0.00))\n",
    "    # inds = inds[keep[inds]]\n",
    "    \n",
    "    # figure(figsize=(3.375, 3.375*7/8), dpi=350)\n",
    "    if do_plot:\n",
    "        # fig = figure(figsize=(8,4), dpi=150, layout=\"constrained\")\n",
    "        # sfigs = fig.subfigures(2, 1, height_ratios=[3, 1])\n",
    "        fig, axes = subplots(2, 1, sharex=True, figsize=(8,4), dpi=150, layout=\"constrained\", gridspec_kw={'height_ratios': [3,1], 'hspace': 0})\n",
    "        sca(axes[0])\n",
    "        # axes[0].set_prop_cycle(cycler('color', ['#006BA4', '#5F9ED1', '#FF800E', '#BBBBBB', '#696969', '#00ff00', '#898989', '#A2C8EC', '#FFBC79', '#CFCFCF']))\n",
    "        axes[0].set_prop_cycle(cycler('color', ['#006BA4', '#FF800E', '#BBBBBB', '#696969', '#00ff00', '#898989', '#A2C8EC', '#FFBC79', '#CFCFCF']))\n",
    "        bin_edges = arange(inds.shape[0]+1) - 0.5\n",
    "        DD = concatenate([zeros_like(lamb)[newaxis,:],\n",
    "                          cumsum(D1 * beta, axis=1).T,\n",
    "                          # cumsum([tlambda(tbeta * torch.eye(N_sigma)[i]).detach().numpy() for i in range(N_sigma)], axis=0),\n",
    "                          lamb.detach().numpy()[newaxis,:]], axis=0)\n",
    "\n",
    "        names = ['eRX', 'PP', 'OB', 'eTX', 'fake pileup']\n",
    "        for i in range(N_sigma+1)[::1]:\n",
    "            fill_between(repeat(bin_edges,2)[1:-1], repeat(DD[i][inds], 2), repeat(DD[i+1][inds], 2), label=names[i], lw=0)\n",
    "\n",
    "        legend(loc='upper right', ncol=2)\n",
    "\n",
    "        errorbar(x=arange(inds.shape[0]), y=Ddata[inds], yerr=yerr(Ddata[inds]) * (Ddata[inds] != 0),\n",
    "                 ls='none', color='k', marker='.', markersize=3, lw=0.6)\n",
    "\n",
    "        xlim(-0.5, inds.shape[0]-0.5)\n",
    "        ylim(1e-2, 5e3)\n",
    "        yscale('log')\n",
    "        grid()\n",
    "        title(plot_title)\n",
    "        ylabel('Incidents')\n",
    "        \n",
    "        sca(axes[1])\n",
    "        errorbar(x=arange(inds.shape[0])[Ddata[inds] != 0], y=(Ddata[inds] / DD[-1][inds])[Ddata[inds] != 0], yerr=(yerr(Ddata[inds]) / DD[-1][inds])[:,Ddata[inds] != 0],\n",
    "                 ls='none', color='k', marker='.', markersize=3, lw=0.6)\n",
    "        ylim(0, 3)\n",
    "        yticks([0,1,2,3])\n",
    "        grid()\n",
    "        ylabel('Data / pred.')\n",
    "        xlabel('Incident category (arbitrary order)')\n",
    "        \n",
    "        # tight_layout()\n",
    "        # show()\n",
    "        if isinstance(do_plot, str):\n",
    "            savefig(f'{do_plot}.png')\n",
    "            savefig(f'{do_plot}.pdf')\n",
    "        show()\n",
    "        close()\n",
    "    \n",
    "    return beta, Cov, H, inds, Ddata, D1, D2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5394a9c5-4dae-4aca-845d-c0fb51022440",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = [analysis(sum(R43[:9]), \"Jan 27, 2024 SEU data: Hexa 43, Runs 1-9\",   D8_43, MC_patt8, M8, sum(fluences[:9]), do_plot='H43_8'),\n",
    "       analysis(sum(R43[9:]), \"Jan 27, 2024 SEU data: Hexa 43, Runs 10-16\", D5_43, MC_patt5, M5, sum(fluences[9:]), do_plot='H43_5'),\n",
    "       analysis(sum(R42[:9]), \"Jan 27, 2024 SEU data: Hexa 42, Runs 1-9\",   D8_42, MC_patt8, M8, sum(fluences[:9]), do_plot='H42_8'),\n",
    "       analysis(sum(R42[9:]), \"Jan 27, 2024 SEU data: Hexa 42, Runs 10-16\", D5_42, MC_patt5, M5, sum(fluences[9:]), do_plot='H42_5'),\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69de532-a48e-4182-ba1f-cb613aee5c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "hexas = [43, 43, 42, 42]\n",
    "runs = ['1-9', '10-16', '1-9', '10-16']\n",
    "names = ['eRX', 'PP', 'OB', 'eTX']\n",
    "for j, (beta, C, H, inds, Ddata, D1, D2) in enumerate(ret):\n",
    "    print(f'Hexa{hexas[j]}, Runs {runs[j]}')\n",
    "    print('='*60)\n",
    "    for i in range(N_sigma):\n",
    "        print(f'\\t{names[i]:>12s} σ = ({beta[i]*1e15:7.2f} ± {sqrt(C[i,i])*1e15:7.2f})×10⁻¹⁵ cm²')\n",
    "    print()\n",
    "    print('\\tCorrelations:')\n",
    "    corr = C / sqrt(outer(diag(C), diag(C)))\n",
    "    for i in range(N_sigma):\n",
    "        print(f'\\t\\t[' + '   '.join([f'{corr[i,j]*100:6.2f}' for j in range(N_sigma)]) + ']')\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f36b7e-a96b-445f-a8f2-579b067cb720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f72fd2c-06e9-4e00-99f9-0d02435a9840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b9b422-ee7a-4a43-8357-9dd070d500e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d15cbd-52c6-4ec2-abe0-a60cc3aba8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11db0eeb-0fbc-4fd5-bd1c-0f14e2a58b81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fad8f3-4908-4834-9500-4e130eec5b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02df218a-859a-434b-bff1-e9342810f4af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45ba407-3bc5-442f-9a16-aff40b5ec35e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bc2076-91cd-45ee-a76d-5d5bce20eb5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca7c3b3-16ef-46bf-acb6-350b29d36b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a41d272-ad24-427a-a59d-1e724a56c582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d46186-0ff5-4257-81dc-5793be1871b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d78ac6-7103-4aeb-b782-8a9bfff91878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ac1e9f-384e-4148-b03a-3aaf48f0cf92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6228960-bd9a-4da8-beae-5df360316b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30e395f-6683-4016-9d40-01e76217f870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f3168b-9a7a-4521-b1f4-a59e27952c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beb1eef-e20e-4827-b36b-19395aa2c669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4cf0e1-b0c2-41d0-8b24-7d6ede0748fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56ffb49-6639-4af3-bd5b-61cae59cb26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "PRBS_J42 = []\n",
    "percaps = []\n",
    "for i, fname in enumerate(hexa42_PRBS_files):\n",
    "    with open(pathlib.Path(hexa_files_location, fname)) as jsonfile:\n",
    "        PRBS_J42.append(json.load(jsonfile))\n",
    "\n",
    "PRBS_J43 = []\n",
    "percaps = []\n",
    "for i, fname in enumerate(hexa43_PRBS_files):\n",
    "    with open(pathlib.Path(hexa_files_location, fname)) as jsonfile:\n",
    "        PRBS_J43.append(json.load(jsonfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf80b1f0-62fc-4df0-8400-b83da939ed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class run_data:\n",
    "    def __init__(self, R, D, MC_patt, M, fluence):\n",
    "        self.R = R\n",
    "        self.D = D\n",
    "        self.MC_patt = MC_patt\n",
    "        self.M = M\n",
    "        self.fluence = fluence\n",
    "        \n",
    "        self.Ddata = zeros(MC_patt.shape[0], int32)\n",
    "        for i, p in enumerate(MC_patt):\n",
    "            self.Ddata[i] += sum(R.incidents.pattern == p)\n",
    "        self.MDdata = (asmatrix(M) * asmatrix(self.Ddata).T).A.flatten()\n",
    "            \n",
    "        self.N_sigma = int((sqrt(1 + 4*D.shape[0]) - 1) / 2)\n",
    "        assert self.N_sigma**2 + self.N_sigma == D.shape[0]\n",
    "        \n",
    "        self.MD = (asmatrix(M) * asmatrix(self.D).T).A.T * fluence\n",
    "        self.D1 = self.MD[:self.N_sigma].T\n",
    "        self.D2 = self.MD[self.N_sigma:].T.reshape(-1, self.N_sigma, self.N_sigma)\n",
    "        \n",
    "        self.tD1 = torch.tensor(self.D1, requires_grad=False)\n",
    "        self.tD2 = torch.tensor(self.D2, requires_grad=False)\n",
    "        self.tDdata = torch.tensor(self.MDdata, requires_grad=False)\n",
    "        \n",
    "    def tlambda(self, tbeta):\n",
    "        return ((self.tD1 * tbeta).sum(axis=1) +\n",
    "                self.fluence * (self.tD2 * torch.outer(tbeta, tbeta)).sum(axis=(1,2)) / (self.tD1 * tbeta).sum())\n",
    "    \n",
    "    def tlogpdf(self, tbeta):\n",
    "        return -torch.distributions.Poisson(self.tlambda(tbeta)).log_prob(self.tDdata).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e7ec37-1552-40ea-a5de-aa9baafd0fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PRBS_run_data:\n",
    "    def __init__(self, Js, fluence, dwelltime, e_bits, e_words, e_specs):\n",
    "        self.N_sigma = 4\n",
    "        self.ASIC = concatenate([array(J['tests'][2]['metadata']['daq_asic_post_beam'], uint32) for J in Js])\n",
    "        self.xor = (self.ASIC ^ numpy.roll(self.ASIC, 1, axis=1)) & (self.ASIC ^ numpy.roll(self.ASIC, -1, axis=1))\n",
    "        self.fluence = fluence\n",
    "        \n",
    "        self.row, self.eTX = self.xor.nonzero()\n",
    "        \n",
    "        self.bit_index = asarray(ceil(log2(self.xor[self.row, self.eTX])), int)\n",
    "        self.err_per_bit = bincount(self.bit_index, minlength=32)\n",
    "\n",
    "        self.terr_per_bit = torch.tensor(self.err_per_bit, requires_grad=False)\n",
    "        \n",
    "        self.PRBS15_errs = sum([J['tests'][2]['metadata']['prbs_15_err_count'][-1][1] for J in Js], axis=0)\n",
    "        self.tPRBS15_errs = torch.tensor(self.PRBS15_errs, requires_grad=False)\n",
    "        \n",
    "        self.dwelltime = dwelltime\n",
    "        \n",
    "        self.dwell7 = array([sum(self.dwelltime[:,(e_bits == i) & (e_specs == 3)], axis=1) for i in range(32)]) * self.fluence\n",
    "        self.tdwell7 = torch.tensor(self.dwell7, requires_grad=False)\n",
    "        \n",
    "        self.dwell15 = array([sum(self.dwelltime[:,(e_words == i) & (e_specs == 0)], axis=1) for i in range(12)]) * self.fluence\n",
    "        self.tdwell15 = torch.tensor(self.dwell15, requires_grad=False)\n",
    "\n",
    "    def tlogpdf(self, tbeta):\n",
    "        trate = self.fluence * (torch.arange(31, -1, -1) + 4) * 6 / 32\n",
    "        return -(torch.distributions.Poisson((self.tdwell7 * tbeta).sum(axis=1)).log_prob(self.terr_per_bit).sum() +\n",
    "                 torch.distributions.Poisson((self.tdwell15 * tbeta).sum(axis=1)).log_prob(self.tPRBS15_errs).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4867949-ba0b-458a-bed2-ed04dc650eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRBS_42 = PRBS_run_data(PRBS_J42, sum(PRBS_fluences), dwelltime42, e_bits, e_words, e_specs)\n",
    "PRBS_43 = PRBS_run_data(PRBS_J43, sum(PRBS_fluences), dwelltime43, e_bits, e_words, e_specs)\n",
    "\n",
    "R8_43 = run_data(sum(R43[:9]), D8_43, MC_patt8, M8, sum(fluences[:9]))\n",
    "R5_43 = run_data(sum(R43[9:]), D5_43, MC_patt5, M5, sum(fluences[9:]))\n",
    "R8_42 = run_data(sum(R42[:9]), D8_42, MC_patt8, M8, sum(fluences[:9]))\n",
    "R5_42 = run_data(sum(R42[9:]), D5_42, MC_patt5, M5, sum(fluences[9:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88fcb2b-6698-436a-b0ca-cf82a7845414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def analysis(Rs):\n",
    "    def tlogpdf(tbeta):\n",
    "        return torch.stack([R.tlogpdf(tbeta) for R in Rs], dim=0).sum()\n",
    "    \n",
    "    def logpdf(beta):\n",
    "        tbeta = torch.tensor(abs(beta))\n",
    "        return tlogpdf(tbeta).detach().numpy().item()\n",
    "\n",
    "    def hesse(beta):\n",
    "        tbeta = torch.tensor(abs(beta), requires_grad=True)\n",
    "        return torch.autograd.functional.hessian(tlogpdf, tbeta).detach().numpy()\n",
    "    \n",
    "    res = scipy.optimize.minimize(logpdf, exp(randn(N_sigma)*0.7) * 1.5e-14,\n",
    "                                  method='Nelder-Mead', options={'fatol': 1e-7, 'xatol':1e-19, 'maxiter': 5000, 'maxfev': 5000, 'disp':False})\n",
    "\n",
    "    # print(res)\n",
    "    # print()\n",
    "    beta = abs(res.x)\n",
    "    H = hesse(beta)\n",
    "    return beta, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0908367-c54a-4f49-93ae-7412baec10b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt(diag(scipy.linalg.inv(H[[0,3]][:,[0,3]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59322372-a006-4199-9430-beccf5116f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_comb = [analysis([R8_42, R5_42, PRBS_42]),\n",
    "        analysis([R8_43, R5_43, PRBS_43])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff85744d-d9ff-4890-82b3-edc96d686322",
   "metadata": {},
   "outputs": [],
   "source": [
    "hexas = [42, 43]\n",
    "runs = ['1-16 + PRBS (Runs 17-19 and 22)']*2\n",
    "names = ['eRX', 'PP', 'OB', 'eTX']\n",
    "for j, (beta, H) in enumerate(ret_comb):\n",
    "    C = scipy.linalg.inv(H)\n",
    "    print(f'Hexa{hexas[j]}, Runs {runs[j]}')\n",
    "    print('='*60)\n",
    "    for i in range(N_sigma):\n",
    "        print(f'\\t{names[i]:>12s} σ = ({beta[i]*1e15:7.2f} ± {sqrt(C[i,i])*1e15:7.2f})×10⁻¹⁵ cm²')\n",
    "    print()\n",
    "    print('\\tCorrelations:')\n",
    "    corr = C / sqrt(outer(diag(C), diag(C)))\n",
    "    for i in range(N_sigma):\n",
    "        print(f'\\t\\t[' + '   '.join([f'{corr[i,j]*100:6.2f}' for j in range(N_sigma)]) + ']')\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80630e1-6ea4-4ea3-b0ee-faa36af78adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_PRBS = [analysis([PRBS_42]),\n",
    "            analysis([PRBS_43])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c76faf-36d2-4b8c-8c43-628935aaa989",
   "metadata": {},
   "outputs": [],
   "source": [
    "hexas = [42, 43]\n",
    "runs = ['PRBS (Runs 17-19 and 22)']*2\n",
    "names = ['eRX', 'eTX']\n",
    "indices = [0, 3]\n",
    "for j, (beta, H) in enumerate(ret_PRBS):\n",
    "    C = scipy.linalg.inv(H[indices][:,indices])\n",
    "    beta = beta[indices]\n",
    "    print(f'Hexa{hexas[j]}, Runs {runs[j]}')\n",
    "    print('='*60)\n",
    "    for i in range(len(indices)):\n",
    "        print(f'\\t{names[i]:>12s} σ = ({beta[i]*1e15:7.2f} ± {sqrt(C[i,i])*1e15:7.2f})×10⁻¹⁵ cm²')\n",
    "    print()\n",
    "    print('\\tCorrelations:')\n",
    "    corr = C / sqrt(outer(diag(C), diag(C)))\n",
    "    for i in range(len(indices)):\n",
    "        print(f'\\t\\t[' + '   '.join([f'{corr[i,j]*100:6.2f}' for j in range(len(indices))]) + ']')\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c1aa55-eb75-4780-882e-db735a95f1a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:coffea]",
   "language": "python",
   "name": "conda-env-coffea-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
